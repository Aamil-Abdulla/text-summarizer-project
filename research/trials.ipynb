{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2dd88150",
   "metadata": {},
   "outputs": [],
   "source": [
    "d={\"key\":\"value\" , \"key2\":\"value2\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d25b828",
   "metadata": {},
   "outputs": [],
   "source": [
    "from box import ConfigBox\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08ca997e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'value'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1=ConfigBox(d)\n",
    "d1.key\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8089dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-03 20:41:00,750 - INFO - 4053448843 - Loading model from artifacts/model_trainer/t5-summarizer\n",
      "2025-12-03 20:41:01,237 - INFO - 4053448843 - Loading tokenizer from artifacts/model_trainer/tokenizer\n",
      "2025-12-03 20:41:01,506 - INFO - 4053448843 - Loading ROUGE metric\n",
      "2025-12-03 20:41:13,445 - INFO - 4053448843 - Loading test dataset from artifacts\\data_ingestion\\samsum-test.csv\n",
      "2025-12-03 20:41:13,821 - INFO - 4053448843 - Calculating metrics on test dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "205it [12:23,  3.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-12-03 20:53:37,232 - INFO - rouge_scorer - Using default tokenizer.\n",
      "2025-12-03 20:53:40,437 - INFO - 4053448843 - Saving metrics to artifacts\\model_evaluation\\metric.csv\n",
      "2025-12-03 20:53:40,453 - INFO - 4053448843 - Metrics saved to artifacts\\model_evaluation\\metric.csv\n",
      "2025-12-03 20:53:40,460 - INFO - 4053448843 - Evaluation complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5TokenizerFast\n",
    "from datasets import Dataset\n",
    "from evaluate import load\n",
    "import yaml\n",
    "from text_summarizer.logging import logger\n",
    "\n",
    "# =======================\n",
    "# CONFIG DATA CLASS\n",
    "# =======================\n",
    "@dataclass\n",
    "class ModelEvaluationConfig:\n",
    "    root_dir: Path\n",
    "    model_path: Path\n",
    "    tokenizer_path: Path\n",
    "    data_path: Path\n",
    "    metric_file_name: Path\n",
    "    column_text: str = \"article\"\n",
    "    column_summary: str = \"highlights\"\n",
    "\n",
    "# =======================\n",
    "# CONFIGURATION MANAGER\n",
    "# =======================\n",
    "class ConfigurationManager:\n",
    "    def __init__(self, config_path: Path):\n",
    "        if not config_path.exists():\n",
    "            raise FileNotFoundError(f\"Config file not found: {config_path}\")\n",
    "        with open(config_path, \"r\") as f:\n",
    "            self.config = yaml.safe_load(f)\n",
    "\n",
    "    def get_model_evaluation_config(self) -> ModelEvaluationConfig:\n",
    "        cfg = self.config[\"model_evaluation\"]\n",
    "        return ModelEvaluationConfig(\n",
    "            root_dir=Path(cfg[\"root_dir\"]),\n",
    "            model_path=Path(cfg[\"model_path\"]),\n",
    "            tokenizer_path=Path(cfg[\"tokenizer_path\"]),\n",
    "            data_path=Path(cfg[\"data_path\"]),\n",
    "            metric_file_name=Path(cfg[\"metric_file_name\"]),\n",
    "            column_text=cfg.get(\"column_text\", \"article\"),\n",
    "            column_summary=cfg.get(\"column_summary\", \"highlights\")\n",
    "        )\n",
    "\n",
    "# =======================\n",
    "# MODEL EVALUATION CLASS\n",
    "# =======================\n",
    "class ModelEvaluation:\n",
    "    def __init__(self, config: ModelEvaluationConfig):\n",
    "        self.config = config\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Load model safely on Windows\n",
    "        model_dir = str(self.config.model_path.as_posix())\n",
    "        tokenizer_dir = str(self.config.tokenizer_path.as_posix())\n",
    "\n",
    "        logger.info(f\"Loading model from {model_dir}\")\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_dir).to(self.device)\n",
    "\n",
    "        logger.info(f\"Loading tokenizer from {tokenizer_dir}\")\n",
    "        self.tokenizer = T5TokenizerFast.from_pretrained(tokenizer_dir)\n",
    "\n",
    "    def generate_batch_sized_chunks(self, list_of_elements, batch_size):\n",
    "        for i in range(0, len(list_of_elements), batch_size):\n",
    "            yield list_of_elements[i:i + batch_size]\n",
    "\n",
    "    def calculate_metrics_on_test_ds(\n",
    "        self, dataset, metric, batch_size=4\n",
    "    ):\n",
    "        self.model.eval()\n",
    "        preds, refs = [], []\n",
    "\n",
    "        for batch in tqdm(self.generate_batch_sized_chunks(list(range(len(dataset))), batch_size)):\n",
    "            texts = [dataset[i][self.config.column_text] for i in batch]\n",
    "            summaries = [dataset[i][self.config.column_summary] for i in batch]\n",
    "\n",
    "            inputs = self.tokenizer(\n",
    "                texts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=512\n",
    "            ).to(self.device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                summary_ids = self.model.generate(\n",
    "                    input_ids=inputs[\"input_ids\"],\n",
    "                    attention_mask=inputs[\"attention_mask\"],\n",
    "                    max_length=150,\n",
    "                    num_beams=2,\n",
    "                    early_stopping=True\n",
    "                )\n",
    "\n",
    "            decoded_preds = self.tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "            preds.extend(decoded_preds)\n",
    "            refs.extend(summaries)\n",
    "\n",
    "        return metric.compute(predictions=preds, references=refs)\n",
    "\n",
    "    def evaluate(self):\n",
    "        logger.info(\"Loading ROUGE metric\")\n",
    "        metric = load(\"rouge\")\n",
    "\n",
    "        # Load test CSV directly\n",
    "        test_csv_path = self.config.data_path\n",
    "        if not test_csv_path.exists():\n",
    "            raise FileNotFoundError(f\"Test CSV not found: {test_csv_path}\")\n",
    "        logger.info(f\"Loading test dataset from {test_csv_path}\")\n",
    "        df = pd.read_csv(test_csv_path)\n",
    "        test_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "        logger.info(\"Calculating metrics on test dataset\")\n",
    "        results = self.calculate_metrics_on_test_ds(test_dataset, metric)\n",
    "         \n",
    "\n",
    "# inside evaluate() before saving\n",
    "        self.config.metric_file_name.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        pd.DataFrame([results]).to_csv(self.config.metric_file_name, index=False)\n",
    "        logger.info(f\"Saving metrics to {self.config.metric_file_name}\")\n",
    "        pd.DataFrame([results]).to_csv(self.config.metric_file_name, index=False)\n",
    "        logger.info(f\"Metrics saved to {self.config.metric_file_name}\")\n",
    "        logger.info(\"Evaluation complete!\")\n",
    "\n",
    "# =======================\n",
    "# USAGE\n",
    "# =======================\n",
    "if __name__ == \"__main__\":\n",
    "    os.chdir(r\"C:\\Users\\Admin\\Desktop\\text-summarizer\\text-summarizer-project\")  # Windows-safe\n",
    "    try:\n",
    "        config_manager = ConfigurationManager(Path(\"config/config.yaml\"))\n",
    "        eval_config = config_manager.get_model_evaluation_config()\n",
    "        evaluator = ModelEvaluation(eval_config)\n",
    "        evaluator.evaluate()\n",
    "    except Exception as e:\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1d1947",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ts",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
